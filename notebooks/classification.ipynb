{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderOil(data.Dataset):\n",
    "    \"\"\"\n",
    "        Dataset class for semantic segmentation tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_path):\n",
    "        super(DataLoaderOil, self).__init__()\n",
    "\n",
    "        self.npy_files  = []\n",
    "        for filename in os.listdir(os.path.join(folder_path, 'pipes_frames')):\n",
    "            ext=os.path.splitext(filename)[1]\n",
    "            if ext not in ['.db']:\n",
    "                self.npy_files.append(os.path.join(folder_path, 'pipes_frames', filename))\n",
    "        \n",
    "        for filename in os.listdir(os.path.join(folder_path, 'spills_frames')):\n",
    "            ext=os.path.splitext(filename)[1]\n",
    "            if ext not in ['.db']:\n",
    "                self.npy_files.append(os.path.join(folder_path, 'spills_frames', filename))\n",
    "                \n",
    "        \n",
    "        print(len(self.npy_files))\n",
    "        \n",
    "        self.start_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.normalize_transform =  transforms.Compose([transforms.Normalize([0],[1])])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        npy_path = self.npy_files[index]\n",
    "        data = np.load(npy_path)\n",
    "        label = 1 if npy_path.find('spills_frames') != -1 else 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        transform_data = self.start_transform(data)\n",
    "        transform_label = label\n",
    "\n",
    "        transform_data = self.normalize_transform(transform_data)\n",
    "        return transform_data, transform_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.npy_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78749\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './dataset'\n",
    "npy_dataset =DataLoaderOil(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "num_train = len(npy_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "np.random.shuffle(indices)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "trainloader = torch.utils.data.DataLoader(npy_dataset, sampler=train_sampler, batch_size=4)\n",
    "testloader = torch.utils.data.DataLoader(npy_dataset, sampler=test_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v3_small(pretrained=True)\n",
    "# model.fc = nn.Linear(2048, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0][0] = nn.Conv2d(19, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[3] = nn.Linear(in_features=1024, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.conv1 = nn.Conv2d(19, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight) \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "optimizer= optim.Adamax([\n",
    "    {'params' : model.parameters()},\n",
    "#     {'params' : model.backbone.layer4.parameters()}\n",
    "    ]\n",
    "    , lr=0.0003)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(full_weights_list).to('cuda'))\n",
    "# criterion=FocalLoss(weight = torch.Tensor([1, 1]).to('cuda'))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 19, 64, 64])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/10.. Train loss: 0.385.. Train accuracy: 82.089.. Test loss: 0.237.. Test accuracy: 84.345\n",
      "1\n",
      "Epoch 2/10.. Train loss: 0.322.. Train accuracy: 85.730.. Test loss: 0.024.. Test accuracy: 78.187\n",
      "2\n",
      "Epoch 3/10.. Train loss: 0.276.. Train accuracy: 88.113.. Test loss: 0.066.. Test accuracy: 85.196\n",
      "3\n",
      "Epoch 4/10.. Train loss: 0.240.. Train accuracy: 89.965.. Test loss: 0.146.. Test accuracy: 87.195\n",
      "4\n",
      "Epoch 5/10.. Train loss: 0.210.. Train accuracy: 91.373.. Test loss: 0.431.. Test accuracy: 90.573\n",
      "5\n",
      "Epoch 6/10.. Train loss: 0.190.. Train accuracy: 92.179.. Test loss: 0.016.. Test accuracy: 90.300\n",
      "6\n",
      "Epoch 7/10.. Train loss: 0.172.. Train accuracy: 93.016.. Test loss: 0.041.. Test accuracy: 92.382\n",
      "7\n",
      "Epoch 8/10.. Train loss: 0.158.. Train accuracy: 93.602.. Test loss: 0.005.. Test accuracy: 92.553\n",
      "8\n",
      "Epoch 9/10.. Train loss: 0.147.. Train accuracy: 94.121.. Test loss: 0.121.. Test accuracy: 93.074\n",
      "9\n",
      "Epoch 10/10.. Train loss: 0.134.. Train accuracy: 94.503.. Test loss: 0.004.. Test accuracy: 94.210\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps = 0\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "running_loss = 0\n",
    "test_acc = 0\n",
    "accuracy = 0\n",
    "train_acc = 0\n",
    "print_every = 100\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "#         print(logps.shape, labels.shape)\n",
    "        loss = criterion(logps, labels.float().unsqueeze(1))\n",
    "        accuracy = binary_acc(logps, labels.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy.item()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    if steps%10000 == 0:\n",
    "        print(steps)\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(inputs)\n",
    "\n",
    "            batch_loss = criterion(logps, labels.float().unsqueeze(1))\n",
    "            accuracy = binary_acc(logps, labels.unsqueeze(1))\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy.item()\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    train_losses.append(running_loss/len(trainloader))\n",
    "    test_losses.append(test_loss/len(testloader))                    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "          f\"Train accuracy: {train_acc/len(trainloader):.3f}.. \"\n",
    "          f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "          f\"Test accuracy: {test_acc/len(testloader):.3f}\")\n",
    "    running_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()   \n",
    "        \n",
    "PATH = f\"./oil_classify_net_{best_acc:.3f}.pth\"\n",
    "torch.save(best_model_wts, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
